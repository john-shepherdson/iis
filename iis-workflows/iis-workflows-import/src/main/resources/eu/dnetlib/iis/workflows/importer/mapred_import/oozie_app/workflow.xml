<workflow-app xmlns='uri:oozie:workflow:0.4' name='importer_mapred_import'>
	
	<parameters>
		<property>
			<name>mapreduce_inputformat_class</name>
			<value>org.apache.hadoop.hbase.mapreduce.TableInputFormat</value>
			<description>class to be used as mapreduce InputFormat</description>
		</property>
		<property>
			<name>hbase_input_table</name>
			<description>source hbase table name from which data should be imported</description>
		</property>
		<property>
			<name>inputformat_idx_props_location</name>
			<value>undefined</value>
			<description>introduced and required for testing purposes only</description>
		</property>
		<property>
			<name>inference_provenance_blacklist</name>
			<value>iis</value>
			<description>list of blacklisted inference provenance which sould not be taken into account by importer, skipped when set to $UNDEFINED$</description>
		</property>
		<property>
			<name>skip_deleted_by_inference</name>
			<value>true</value>
			<description>flag indicating records deleted by inference should be skipped</description>
		</property>
		<property>
			<name>trust_level_threshold</name>
			<value>$UNDEFINED$</value>
			<description>trust level threshold represented as float value, ignored when set to $UNDEFINED$ value</description>
		</property>
		<property>
			<name>merge_body_with_updates</name>
			<value>false</value>
			<description>flag indicating Oaf objects strored in body qualifier should be merged with Oaf objects stored in update qualifier</description>
		</property>
		<property>
			<name>hbase_table_encoding</name>
			<value>UTF-8</value>
			<description>hbase table encoding, usually UTF-8</description>
		</property>
		<!-- disabling, currently content import is conducted by dedicated module -->
		<!-- 
		<property>
			<name>object_store_location</name>
			<value>$UNDEFINED$</value>
			<description>object store service location required for content retrieval</description>
		</property>
		<property>
			<name>lookup_service_location</name>
			<value>$UNDEFINED$</value>
			<description>lookup service location required for content retrieval, finding object store id based on repository id</description>
		</property>
		 -->
		<property>
			<name>approved_datasources_csv</name>
			<value>$UNDEFINED$</value>
			<description>CSV list of datasource ids to be approved during import. Applied on result and person entities.</description>
		</property>
		
		<property>
			<name>hbase_remote_zookeeper_quorum</name>
			<description>remote zookeeper quorum location, disabled when set to $UNDEFINED$ value</description>
		</property>
		<property>
			<name>hbase_remote_zookeeper_clientport</name>
			<description>remote zookeeper client port, disabled when set to $UNDEFINED$ value</description>
		</property>
		<property>
			<name>output</name>
			<description>import output root directory</description>
		</property>
		<!-- output subdirectory names -->
		<property>
			<name>output_name_document_meta</name>
			<value>docmeta</value>
			<description>document metadata output subdirectory name</description>
		</property>
		<property>
			<name>output_name_citation</name>
			<value>citation</value>
			<description>citation output subdirectory name</description>
		</property>
		<property>
			<name>output_name_document_project</name>
			<value>docproject</value>
			<description>document to project relation output subdirectory name</description>
		</property>
		<property>
			<name>output_name_project</name>
			<value>project</value>
			<description>project output subdirectory name</description>
		</property>
		<property>
			<name>output_name_person</name>
			<value>person</value>
			<description>person output subdirectory name</description>
		</property>
		<property>
			<name>output_name_dedup_mapping</name>
			<value>dedupmapping</value>
			<description>deduplication mapping output subdirectory name</description>
		</property>
	</parameters>

	<global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapred.job.queue.name</name>
                <value>${queueName}</value>
            </property>
		</configuration>
	</global>

	<start to="generate-schema" />

	<action name="generate-schema">
	    <java>
	        <main-class>eu.dnetlib.iis.common.javamapreduce.hack.AvroSchemaGenerator</main-class>
			<arg>eu.dnetlib.iis.importer.schemas.DocumentMetadata</arg>
			<arg>eu.dnetlib.iis.citationmatching.schemas.Citation</arg>
			<arg>eu.dnetlib.iis.importer.schemas.DocumentToProject</arg>
			<arg>eu.dnetlib.iis.common.schemas.IdentifierMapping</arg>
			<arg>eu.dnetlib.iis.importer.schemas.Person</arg>
			<arg>eu.dnetlib.iis.importer.schemas.Project</arg>
	        <capture-output />
	    </java>
	    <ok to="get-scanner" />
	    <error to="fail" />
	</action>
	
	<action name='get-scanner'>
		<java>
			<main-class>eu.dnetlib.iis.workflows.importer.mapred.helper.ScanStringGenerator</main-class>
			<!--
			start 
			<arg>-s datasource|2a78ddb2-3ef4-4304-97e2-827951fde71e</arg>
			end
			<arg>-e organization|JORG_Q29wZXJuaWN1cyBHZXNlbGxzY2hhZnQgbWJIIChUQyk=</arg>
			-->
			<!-- 
			column family:
			 -->
			<arg>-f person, project, result, resultProject_outcome_isProducedBy, personResult_authorship_hasAuthor, resultResult_dedup_merges</arg>
			<capture-output />
		</java>
		<ok to="mr_import" />
		<error to="fail" />
	</action>
	
	<action name='mr_import'>
		<map-reduce>
			<prepare>
				<delete path="${nameNode}${output}" />
			</prepare>
			<configuration>
				<property>
					<name>hbase.zookeeper.quorum</name>
					<value>${hbase_remote_zookeeper_quorum}</value>
				</property>
				<property>
					<name>hbase.zookeeper.property.clientPort</name>
					<value>${hbase_remote_zookeeper_clientport}</value>
				</property>
				<!-- This is required for new api usage -->
				<property>
					<name>mapred.mapper.new-api</name>
					<value>true</value>
				</property>
				<property>
					<name>mapred.reducer.new-api</name>
					<value>true</value>
				</property>
				<!-- HBASE CONFIGURATIONS -->
				<property>
					<name>hbase.mapreduce.inputtable</name>
					<value>${hbase_input_table}</value>
				</property>
				<property>
					<name>hbase.mapreduce.scan</name>
					<value>${wf:actionData('get-scanner')['scan']}</value>
				</property>
				<!-- MAPPER CONFIGURATIONS -->
				<property>
					<!-- this property is required for PredefinedTableInputFormat -->
					<!-- has to be declared here otherwise integration test will not work,
						try to find better solution -->
					<name>import.input.format.idx.props.location</name>
					<value>${inputformat_idx_props_location}</value>
				</property>
				<!-- disabling speculative execution for HBase reads on map phase -->
				<property>
					<name>mapred.map.tasks.speculative.execution</name>
					<value>false</value>
				</property>
				<property>
					<!-- supported since hadoop 2.3.0 -->
					<name>mapreduce.map.speculative</name>
					<value>false</value>
				</property>
				<property>
					<name>mapreduce.inputformat.class</name>
					<value>${mapreduce_inputformat_class}</value>
				</property>
				<property>
					<name>mapred.mapoutput.key.class</name>
					<value>org.apache.avro.mapred.AvroKey</value>
				</property>
				<property>
					<name>mapred.mapoutput.value.class</name>
					<value>org.apache.avro.mapred.AvroValue</value>
				</property>
				<property>
					<name>mapreduce.map.class</name>
					<value>eu.dnetlib.iis.workflows.importer.mapred.IISDataImporterMapper</value>
				</property>
				
				<property>
					<name>mapred.output.key.class</name>
					<value>org.apache.avro.mapred.AvroKey</value>
				</property>
				<property>
					<name>mapred.output.value.class</name>
					<value>org.apache.avro.mapred.AvroValue</value>
				</property>
				<property>
					<name>mapred.output.key.comparator.class</name>
					<value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
				</property>
				<property>
					<name>io.serializations</name>
					<value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.avro.hadoop.io.AvroSerialization
					</value>
				</property>
				<property>
					<name>mapred.output.value.groupfn.class</name>
					<value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
				</property>
				<property>
					<name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB
					</name>
					<value>org.apache.hadoop.ipc.ProtobufRpcEngine</value>
				</property>
				<property>
					<name>mapred.output.dir</name>
					<value>${output}</value>
				</property>

				<!-- # Job-specific options -->
		        <property>
		            <name>import.inference.provenance.blacklist</name>
		            <value>${inference_provenance_blacklist}</value>
		        </property>
		        <property>
		            <name>import.skip.deleted.by.inference</name>
		            <value>${skip_deleted_by_inference}</value>
		        </property>
		        <property>
		            <name>import.trust.level.threshold</name>
		            <value>${trust_level_threshold}</value>
		        </property>
		        <property>
		            <name>import.merge.body.with.updates</name>
		            <value>${merge_body_with_updates}</value>
		        </property>
		        <property>
		            <name>hbase.table.encoding</name>
		            <value>${hbase_table_encoding}</value>
		        </property>
		        <!-- subdirectory names -->
		        <property>
		            <name>output.name.document_meta</name>
		            <value>${output_name_document_meta}</value>
		        </property>
		        <property>
		            <name>output.name.citation</name>
		            <value>${output_name_citation}</value>
		        </property>
		        <property>
		            <name>output.name.document_project</name>
		            <value>${output_name_document_project}</value>
		        </property>
		        <property>
		            <name>output.name.project</name>
		            <value>${output_name_project}</value>
		        </property>
		        <property>
		            <name>output.name.person</name>
		            <value>${output_name_person}</value>
		        </property>
		        <property>
		            <name>output.name.dedup_mapping</name>
		            <value>${output_name_dedup_mapping}</value>
		        </property>
		        <property>
		            <name>import.approved.datasources.csv</name>
		            <value>${approved_datasources_csv}</value>
		        </property>
				
				<!-- ## Names of all output ports -->
				<property>
					<name>avro.mapreduce.multipleoutputs</name>
					<value>${output_name_document_meta} ${output_name_citation} ${output_name_document_project} ${output_name_dedup_mapping} ${output_name_person} ${output_name_project}</value>
				</property>
				<!-- ## Output classes for all output ports -->
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_document_meta}.format
					</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_citation}.format
					</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_document_project}.format
					</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_dedup_mapping}.format
					</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_person}.format
					</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_project}.format
					</name>
					<value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
				</property>
				
				<!-- ### Schema of multiple output ports. -->
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_document_meta}.keyschema</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.importer.schemas.DocumentMetadata']}</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_citation}.keyschema</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.citationmatching.schemas.Citation']}</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_document_project}.keyschema</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.importer.schemas.DocumentToProject']}</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_dedup_mapping}.keyschema</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.common.schemas.IdentifierMapping']}</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_person}.keyschema</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.importer.schemas.Person']}</value>
				</property>
				<property>
					<name>avro.mapreduce.multipleoutputs.namedOutput.${output_name_project}.keyschema</name>
					<value>${wf:actionData('generate-schema')['eu.dnetlib.iis.importer.schemas.Project']}</value>
				</property>
				<property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
			</configuration>
		</map-reduce>
		<ok to="end" />
		<error to="fail" />
	</action>
	<kill name="fail">
		<message>Map/Reduce failed, error
			message[${wf:errorMessage(wf:lastErrorNode())}]</message>
	</kill>
	<end name='end' />
</workflow-app>