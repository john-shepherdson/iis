<?xml version="1.0"?>
<!-- TODO MiconCodeReview: From external project user viewpoint, it would be more appropriate to call this a 'main' instead of 'chain' workflow. -->
<!-- TODO MiconCodeReview: E.g.: the main workflow in icm-iis-collapsers is called just 'main'. -->
<!-- TODO MiconCodeReview: Also, 'citationmatching_' prefix is unnecessary (it's encoded in the path eu.dnetlib.iis.citationmatching). -->
<!-- TODO MiconCodeReview: In case of renaming, the package should be renamed accordingly ('chain' -> 'main'). -->
<workflow-app xmlns="uri:oozie:workflow:0.4" name="citationmatching_chain">

	<parameters>
        <property>
            <!-- TODO MiconCodeReview: Property name could be more meaningful, like 'inputMetadata'. -->
            <name>input</name>
            <description>input directory holding eu.dnetlib.iis.citationmatching.schemas.DocumentMetadata avro datastore</description>
        </property>
        <property>
            <!-- TODO MiconCodeReview: Property name could be more meaningful, like 'outputCitations'. -->
            <name>output</name>
            <description>output directory holding eu.dnetlib.iis.citationmatching.schemas.Citation avro datastore</description>
        </property>
        <property>
            <name>reduceTasks</name>
            <value>30</value>
        </property>
        <property>
            <name>remove_sideproducts</name>
            <value>true</value>
        </property>
    </parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>io.serializations</name>
                <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.avro.hadoop.io.AvroSerialization
                </value>
            </property>
            <property>
                <name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB
                </name>
                <value>org.apache.hadoop.ipc.ProtobufRpcEngine</value>
            </property>
            <!-- ## This is required for new MapReduce API usage -->
            <property>
                <name>mapred.mapper.new-api</name>
                <value>true</value>
            </property>
            <property>
                <name>mapred.reducer.new-api</name>
                <value>true</value>
            </property>
        </configuration>
    </global>

    <start to="citation-matching" />

    <action name="citation-matching">
    
        <spark xmlns="uri:oozie:spark-action:0.1">

            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
           
            <prepare>
                <delete path="${output}" />
            </prepare>
            
            <master>yarn-cluster</master>
            <mode>cluster</mode>
            <name>citation-matching</name>

            <class>pl.edu.icm.coansys.citations.CitationMatchingJob</class>

            <jar>${oozieTopWfApplicationPath}/lib/iis-workflows-citationmatching-${projectVersion}.jar</jar>
        
            <arg>-citationPath = ${input}</arg>
            <arg>-documentPath = ${input}</arg>
            
            <arg>-hashGeneratorClasses = pl.edu.icm.coansys.citations.hashers.CitationNameYearPagesHashGenerator:pl.edu.icm.coansys.citations.hashers.DocumentNameYearPagesHashGenerator</arg>
            <arg>-hashGeneratorClasses = pl.edu.icm.coansys.citations.hashers.CitationNameYearPagesHashGenerator:pl.edu.icm.coansys.citations.hashers.DocumentNameYearNumNumHashGenerator</arg>
            <arg>-hashGeneratorClasses = pl.edu.icm.coansys.citations.hashers.CitationNameYearHashGenerator:pl.edu.icm.coansys.citations.hashers.DocumentNameYearStrictHashGenerator</arg>
            <arg>-hashGeneratorClasses = pl.edu.icm.coansys.citations.hashers.CitationNameYearHashGenerator:pl.edu.icm.coansys.citations.hashers.DocumentNameYearHashGenerator</arg>
            
            <arg>-outputDirPath = ${output}</arg>
            <arg>-maxHashBucketSize = 10000</arg>
            <arg>-numberOfPartitions = 5</arg>
            
            <arg>-inputDocumentReaderClass = eu.dnetlib.iis.workflows.citationmatching.DocumentMetadataInputReader</arg>
            <arg>-inputDocumentConverterClass = eu.dnetlib.iis.workflows.citationmatching.DocumentMetadataInputConverter</arg>
            
            <arg>-inputCitationReaderClass = eu.dnetlib.iis.workflows.citationmatching.ReferenceMetadataInputReader</arg>
            <arg>-inputCitationConverterClass = eu.dnetlib.iis.workflows.citationmatching.ReferenceMetadataInputConverter</arg>
            
            <arg>-outputConverterClass = eu.dnetlib.iis.workflows.citationmatching.CitationOutputConverter</arg>
            <arg>-outputWriterClass = eu.dnetlib.iis.workflows.citationmatching.CitationOutputWriter</arg>
            
        </spark>
        <ok to="end"/>
        <error to="fail"/>
    
    </action>
    
    
    <kill name="fail">
        <message>Unfortunately, the process failed -- error message:
            [${wf:errorMessage(wf:lastErrorNode())}]
        </message>
    </kill>
    <end name="end" />
</workflow-app>
