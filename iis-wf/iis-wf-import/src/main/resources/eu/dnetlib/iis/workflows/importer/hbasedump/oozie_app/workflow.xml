<?xml version="1.0"?>
<workflow-app xmlns="uri:oozie:workflow:0.4" name="import_hbasedump">

	<parameters>
		<property>
			<name>input</name>
			<description>Source hdfs directory holding hbase dump.
				The dump has a custom format (as exported from Info Space) and is
				enclosed in a sequence file.
			</description>
		</property>
		<property>
			<name>output_table_name</name>
			<description>hbase destination table name</description>
		</property>
		<property>
			<name>hbase_remote_zookeeper_quorum</name>
			<description>hbase zookeeper quorum, have to be provided explicitly</description>
		</property>
		<property>
			<name>hbase_remote_zookeeper_clientport</name>
			<description>external hbase zookeeper client port, have to be provided explicitly</description>
		</property>
	</parameters>

	<global>
		<job-tracker>${jobTracker}</job-tracker>
		<name-node>${nameNode}</name-node>
		<configuration>
			<property>
				<name>mapred.mapper.new-api</name>
				<value>true</value>
			</property>
			<property>
				<name>mapred.reducer.new-api</name>
				<value>true</value>
			</property>
			<property>
				<name>mapred.job.queue.name</name>
				<value>${queueName}</value>
			</property>
		</configuration>
	</global>

	<start to="import" />

	<action name="import">
		<map-reduce>
			<configuration>
				<property>
					<name>mapreduce.inputformat.class</name>
					<value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat
					</value>
				</property>
				<property>
					<name>mapreduce.outputformat.class</name>
					<value>org.apache.hadoop.hbase.mapreduce.TableOutputFormat</value>
				</property>
				<property>
					<name>mapred.mapoutput.key.class</name>
					<value>org.apache.hadoop.hbase.io.ImmutableBytesWritable</value>
				</property>
				<property>
					<name>mapred.mapoutput.value.class</name>
					<value>org.apache.hadoop.hbase.client.Put</value>
				</property>
				<property>
					<name>mapreduce.map.class</name>
					<value>eu.dnetlib.iis.workflows.importer.hbasedump.ImportInformationSpaceMapper
					</value>
				</property>
				<property>
					<name>mapred.reduce.tasks</name>
					<value>0</value>
				</property>
				<property>
					<name>mapred.input.dir</name>
					<value>${input}</value>
				</property>
				<property>
					<name>hbase.mapred.outputtable</name>
					<value>${output_table_name}</value>
				</property>
				<property>
					<name>hbase.mapreduce.outputtable</name>
					<value>${output_table_name}</value>
				</property>
				<property>
					<name>hbase.zookeeper.quorum</name>
					<value>${hbase_remote_zookeeper_quorum}</value>
				</property>
				<property>
					<name>hbase.zookeeper.property.clientPort</name>
					<value>${hbase_remote_zookeeper_clientport}</value>
				</property>
				<!-- deprecated since cdh5 -->
				<property>
					<name>mapred.map.tasks.speculative.execution</name>
					<value>false</value>
				</property>
				<!-- supported since hadoop 2.3.0 -->
				<property>
					<name>mapreduce.map.speculative</name>
					<value>false</value>
				</property>
			</configuration>
		</map-reduce>
		<ok to="end" />
		<error to="fail" />
	</action>

	<kill name="fail">
		<message>Unfortunately, the process failed -- error message:
			[${wf:errorMessage(wf:lastErrorNode())}]
		</message>
	</kill>
	<end name="end" />
</workflow-app>
