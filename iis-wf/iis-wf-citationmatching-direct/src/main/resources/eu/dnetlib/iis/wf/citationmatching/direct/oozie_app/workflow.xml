<workflow-app xmlns="uri:oozie:workflow:0.4" name="citationmatching_direct">

	<parameters>
		<property>
			<name>input</name>
			<description>input containing document metadata records</description>
		</property>
		<property>
			<name>output</name>
			<description>extracted citations</description>
		</property>
        <property>
            <name>sparkExecutorMemory</name>
            <value>7G</value>
            <description>memory for individual executor</description>
        </property>
        <property>
            <name>sparkExecutorCores</name>
            <value>1</value>
            <description>number of cores used by single executor</description>
        </property>
        <property>
            <name>sparkNumExecutors</name>
            <value>200</value>
            <description>total number of executors</description>
        </property>
	</parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${queueName}</value>
            </property>
            <property>
                <name>oozie.launcher.mapred.job.queue.name</name>
                <value>${oozieLauncherQueueName}</value>
            </property>
        </configuration>
    </global>
    
    
	<start to="citationmatchig-direct" />

	<action name="citationmatchig-direct">
		<spark xmlns="uri:oozie:spark-action:0.1">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<!-- The data generated by this node is deleted in this section -->
			<prepare>
				<delete path="${nameNode}${output}" />
			</prepare>

			<master>yarn-cluster</master>
			<mode>cluster</mode>
			<name>citationmatching_direct_input_transformer</name>
			<class>eu.dnetlib.iis.wf.citationmatching.direct.CitationMatchingDirectJob</class>
			<jar>${oozieTopWfApplicationPath}/lib/iis-wf-citationmatching-direct-${projectVersion}-uber.jar</jar>
            
            <spark-opts>--executor-memory ${sparkExecutorMemory} --executor-cores ${sparkExecutorCores} --num-executors ${sparkNumExecutors}</spark-opts>

			<arg>-inputAvroPath=${input}</arg>
			<arg>-outputAvroPath=${output}</arg>

		</spark>
		<ok to="end" />
		<error to="fail" />
	</action>
	
	<kill name="fail">
		<message>Unfortunately, the workflow failed -- error message:
			[${wf:errorMessage(wf:lastErrorNode())}]
		</message>
	</kill>
	<end name="end" />
</workflow-app>
