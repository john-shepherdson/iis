<?xml version="1.0"?>
<workflow-app xmlns="uri:oozie:workflow:0.4" name="documentsclassification_main">
	
	<parameters>
		<property>
			<name>input_documents</name>
			<description>input source documents</description>
		</property>
		<property>
			<name>output_document_to_document_classes</name>
			<description>output document to document classes</description>
		</property>
        <property>
            <name>output_report_root_path</name>
            <description>base directory for storing reports</description>
        </property>
        <property>
            <name>output_report_relative_path</name>
            <value>document_classification</value>
            <description>directory for storing report (relative to output_report_root_path)</description>
        </property>
        <property>
            <name>sparkDriverMemory</name>
            <description>memory for driver process</description>
        </property>
        <property>
            <name>sparkExecutorMemory</name>
            <description>memory for individual executor</description>
        </property>
        <property>
            <name>sparkExecutorCores</name>
            <description>number of cores used by single executor</description>
        </property>
        <property>
            <name>numberOfPartitions</name>
            <value>$UNDEFINED$</value>
            <description>number of partitions the input data should be sliced into. 
            Affects the number of tasks executed on the input data and each task execution time (more tasks less time each executor takes). 
            Relying on default value when $UNDEFINED$.</description>
        </property>
    </parameters>

    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${queueName}</value>
            </property>
            <property>
                <name>oozie.launcher.mapred.job.queue.name</name>
                <value>${oozieLauncherQueueName}</value>
            </property>
        </configuration>
    </global>

    <start to="document_classification_job" />

	<action name="document_classification_job"> 
        <spark xmlns="uri:oozie:spark-action:0.2">
            
            <master>yarn-cluster</master>
            
            <mode>cluster</mode>
            
            <name>document_classification_job</name>
            
            <class>eu.dnetlib.iis.wf.documentsclassification.DocumentClassificationJob</class>
            <jar>${oozieTopWfApplicationPath}/lib/iis-wf-documentsclassification-${projectVersion}.jar</jar>
            
            <spark-opts>
                --executor-memory=${sparkExecutorMemory}
                --executor-cores=${sparkExecutorCores}
                --driver-memory=${sparkDriverMemory}
            </spark-opts>
            
            <arg>-inputAvroPath=${input_documents}</arg>
            <arg>-outputAvroPath=${output_document_to_document_classes}</arg>
            <arg>-scriptDirPath=${wf:conf('oozie.wf.application.path')}/lib/scripts</arg>
            <arg>-numberOfPartitions=${numberOfPartitions}</arg>
            
            <arg>-outputReportPath=${output_report_root_path}/${output_report_relative_path}</arg>
            
        </spark>
        <ok to="end"/>
        <error to="fail"/>
    </action>

    <kill name="fail">
        <message>Unfortunately, the process failed -- error message:
            [${wf:errorMessage(wf:lastErrorNode())}]
        </message>
    </kill>

    <end name="end"/>

</workflow-app>
