<?xml version="1.0"?>
<workflow-app xmlns="uri:oozie:workflow:0.4" name="citation_collapser">

    <parameters>
        <property>
            <name>input</name>
            <description>input main port</description>
        </property>
        <property>
            <name>output</name>
            <description>output port</description>
        </property>
        <property>
            <name>output_report_root_path</name>
            <description>base directory for storing reports</description>
        </property>
        <property>
            <name>output_report_relative_path</name>
            <value>citation</value>
            <description>directory for storing report (relative to output_report_root_path)</description>
        </property>
        <property>
            <name>mapred_child_java_opts</name>
            <value>-Xmx2048M</value>
            <description>java-opts, e.g. maximum heap size for a single JVM running MapReduce</description>
        </property>
    </parameters>


    <global>
        <job-tracker>${jobTracker}</job-tracker>
        <name-node>${nameNode}</name-node>
        <configuration>
            <property>
                <name>mapreduce.job.queuename</name>
                <value>${queueName}</value>
            </property>
            <property>
                <name>oozie.launcher.mapred.job.queue.name</name>
                <value>${oozieLauncherQueueName}</value>
            </property>
        </configuration>
    </global>


    <start to="generate-schema" />

    <action name="generate-schema">
        <java>
            <main-class>eu.dnetlib.iis.common.javamapreduce.hack.AvroSchemaGenerator</main-class>
            <arg>eu.dnetlib.iis.common.citations.schemas.Citation</arg>
            <capture-output />
        </java>
        <ok to="collapser" />
        <error to="fail" />
    </action>

    <action name="collapser">
        <map-reduce>
            <!-- The data generated by this node is deleted in this section -->
            <prepare>
                <delete path="${nameNode}${output}" />
                <delete path="${nameNode}${workingDir}/collapser" />
                <mkdir path="${nameNode}${workingDir}/collapser" />
            </prepare>
            <configuration>
                <property>
                    <name>mapred.child.java.opts</name>
                    <value>${mapred_child_java_opts}</value>
                </property>
                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.avro.mapreduce.AvroKeyInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.avro.mapreduce.AvroKeyOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.map.output.key.class</name>
                    <value>org.apache.avro.mapred.AvroKey</value>
                </property>
                <property>
                    <name>mapreduce.map.output.value.class</name>
                    <value>org.apache.avro.mapred.AvroValue</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.avro.mapred.AvroKey</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.avro.mapred.AvroValue</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.comparator.class</name>
                    <value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
                </property>
                <property>
                    <name>io.serializations</name>
                    <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization,org.apache.avro.hadoop.io.AvroSerialization
                    </value>
                </property>
                <property>
                    <name>mapreduce.job.output.group.comparator.class</name>
                    <value>org.apache.avro.hadoop.io.AvroKeyComparator</value>
                </property>
                <property>
                    <name>rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB
                    </name>
                    <value>org.apache.hadoop.ipc.ProtobufRpcEngine</value>
                </property>

                <!-- ## This is required for new MapReduce API usage -->

                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>

                <!-- # Job-specific options -->

                <!-- ## Classes of mapper and reducer -->

                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>eu.dnetlib.iis.wf.collapsers.GroupByFieldMapper</value>
                </property>
                <property>
                    <name>mapreduce.job.reduce.class</name>
                    <value>eu.dnetlib.iis.wf.collapsers.CollapserReducer</value>
                </property>

                <!-- ## Schemas -->

                <!-- ### Schema of the data ingested by the mapper. To be more precise, it's the schema of Avro data passed as template parameter of the AvroKey object passed to mapper -->
                <property>
                    <name>avro.schema.input.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.common.citations.schemas.Citation']}</value>
                </property>

                <!-- ### Schemas of the data produced by the mapper -->

                <!-- #### Schema of the key produced by the mapper. To be more precise, it's the schema of Avro data produced by the mapper and passed forward as template parameter of AvroKey object. -->
                <property>
                    <name>avro.serialization.key.reader.schema</name>
                    <value>"string"</value>
                </property>
                <property>
                    <name>avro.serialization.key.writer.schema</name>
                    <value>"string"</value>
                </property>
                <!-- #### Schema of the value produced by the mapper. To be more precise, it's the schema of Avro data produced by the mapper and passed forward as template parameter of AvroValue object. -->
                <property>
                    <name>avro.serialization.value.reader.schema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.common.citations.schemas.Citation']}</value>
                </property>
                <property>
                    <name>avro.serialization.value.writer.schema</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.common.citations.schemas.Citation']}</value>
                </property>

                <property>
                    <name>collapser.reducer.schema.class</name>
                    <value>eu.dnetlib.iis.common.citations.schemas.Citation</value>
                </property>
                <!-- ### Schema of the data produced by the reducer -->
                <property>
                    <name>avro.schema.output.key</name>
                    <value>${wf:actionData('generate-schema')['eu.dnetlib.iis.common.citations.schemas.Citation']}</value>
                </property>
                <!-- ## Specification of the input and output data store -->
                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${input}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${output}</value>
                </property>
                <!-- ## Workflow node parameters -->
                <property>
                    <name>blocking_field</name>
                    <value>sourceDocumentId</value>
                </property>
                <property>
                    <name>record_collapser</name>
                    <value>eu.dnetlib.iis.wf.collapsers.basic.GenericCitationCollapser</value>
                </property>
                <!-- report related parameters -->
                <property>
                    <name>oozie.action.external.stats.write</name>
                    <value>true</value>
                </property>
            </configuration>
        </map-reduce>
        <ok to="report" />
        <error to="fail" />
    </action>

    <action name="report">
        <java>
            <main-class>eu.dnetlib.iis.common.java.ProcessWrapper</main-class>
            <arg>eu.dnetlib.iis.common.report.ReportGenerator</arg>
            <arg>-Preport.processing.citationTextExtraction.citation=${hadoop:counters('collapser')['eu.dnetlib.iis.wf.collapsers.basic.GenericCitationCollapser$CitationTextCounters']['TOTAL']}</arg>
            <arg>-Preport.processing.citationTextExtraction.doc=${hadoop:counters('collapser')['eu.dnetlib.iis.wf.collapsers.basic.GenericCitationCollapser$CitationTextCounters']['DOCS_WITH_AT_LEAST_ONE_CITATION']}
            </arg>
            <arg>-Oreport=${output_report_root_path}/${output_report_relative_path}</arg>
        </java>
        <ok to="end" />
        <error to="fail" />
    </action>

    <kill name="fail">
        <message>Unfortunately, the process failed -- error message:
            [${wf:errorMessage(wf:lastErrorNode())}]
        </message>
    </kill>

    <end name="end" />

</workflow-app>
